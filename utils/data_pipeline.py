#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DataPipeline: ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤

ë°ì´í„° ë¡œë“œ, ë¶„ì„, í•„í„°ë§, ì €ì¥ ë“± ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„
ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""

import json
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
from collections import Counter

from utils.morph_analyzer import MorphAnalyzer


class DataPipeline:
    """ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, max_workers=20):
        """
        DataPipeline ì´ˆê¸°í™”
        
        Args:
            max_workers (int): ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
        """
        self.analyzer = MorphAnalyzer()
        self.max_workers = max_workers
        self.df = None
    
    def load_data(self, file_path):
        """
        CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            file_path (str or Path): CSV íŒŒì¼ ê²½ë¡œ
            
        Returns:
            DataPipeline: ë©”ì„œë“œ ì²´ì´ë‹ì„ ìœ„í•œ self ë°˜í™˜
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"âŒ ì˜¤ë¥˜: {file_path}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        print(f"\\në°ì´í„° ë¡œë“œ ì¤‘: {file_path}")
        self.df = pd.read_csv(file_path)
        print(f"ë¡œë“œëœ í–‰ ìˆ˜: {len(self.df)}")
        
        return self
    
    def analyze_morphology(self):
        """
        í…ìŠ¤íŠ¸ì— ëŒ€í•´ í˜•íƒœì†Œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Returns:
            DataPipeline: ë©”ì„œë“œ ì²´ì´ë‹ì„ ìœ„í•œ self ë°˜í™˜
        """
        if self.df is None:
            raise ValueError("ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_data()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        print("\\n" + "=" * 60)
        print("í˜•íƒœì†Œ ë¶„ì„, ë¬¸í˜• ë¶„ë¥˜, ê¸°í˜¸ ì¶”ì¶œ ì¤‘ (Bareun - ë³‘ë ¬ ì²˜ë¦¬)...")
        print("=" * 60)
        
        texts = self.df['full_text'].tolist()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            results = list(tqdm(
                executor.map(self.analyzer.analyze_text, texts),
                total=len(texts),
                desc="ë¶„ì„ ì§„í–‰"
            ))
        
        # ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ì €ì¥
        self.df['sentence_results'] = [json.dumps(r, ensure_ascii=False) for r in results]
        
        return self
    
    def filter_banmal(self):
        """
        ë°˜ë§ ê²Œì‹œê¸€ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
        
        Returns:
            DataPipeline: ë©”ì„œë“œ ì²´ì´ë‹ì„ ìœ„í•œ self ë°˜í™˜
        """
        if self.df is None:
            raise ValueError("ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if 'sentence_results' not in self.df.columns:
            raise ValueError("í˜•íƒœì†Œ ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. analyze_morphology()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        print("\\në°˜ë§ ê¸€ í•„í„°ë§ ì¤‘...")
        initial_count = len(self.df)
        
        # sentence_resultsì—ì„œ ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ endingsë¥¼ ì¶”ì¶œí•˜ì—¬ ë°˜ë§ ì—¬ë¶€ íŒë³„
        def check_banmal(sentence_results_json):
            try:
                results = json.loads(sentence_results_json)
                if results and len(results) > 0:
                    endings = results[0].get('endings', [])
                    return MorphAnalyzer.is_banmal(endings)
                return False
            except:
                return False
        
        self.df['is_banmal'] = self.df['sentence_results'].apply(check_banmal)
        self.df = self.df[self.df['is_banmal'] == True].copy()
        
        removed_count = initial_count - len(self.df)
        print(f"ì œê±°ëœ ì¡´ëŒ“ë§ ê¸€: {removed_count}ê°œ")
        print(f"ë‚¨ì€ ë°˜ë§ ê¸€: {len(self.df)}ê°œ")
        
        return self
    
    def report_neologisms(self, neologisms=['ê¸”', 'ë…¸', 'ìŠ¨']):
        """
        ì‹ ì¡°ì–´ ì¢…ê²° ì–´ë¯¸ ë¹ˆë„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
        
        Args:
            neologisms (list): í™•ì¸í•  ì‹ ì¡°ì–´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            DataPipeline: ë©”ì„œë“œ ì²´ì´ë‹ì„ ìœ„í•œ self ë°˜í™˜
        """
        if self.df is None or 'sentence_results' not in self.df.columns:
            raise ValueError("ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        print("\\n" + "=" * 60)
        print("ì‹ ì¡°ì–´ ì¢…ê²° ì–´ë¯¸ í™•ì¸ ì¤‘...")
        neo_counts = {neo: 0 for neo in neologisms}
        
        for sentence_results_json in self.df['sentence_results']:
            try:
                results = json.loads(sentence_results_json)
                for sent_res in results:
                    for morph, tag in sent_res.get('endings', []):
                        if morph in neologisms:
                            neo_counts[morph] += 1
            except:
                continue
        
        print("\\nì‹ ì¡°ì–´ ë¹ˆë„:")
        for neo, count in neo_counts.items():
            print(f"  '{neo}': {count}íšŒ")
        
        return self
    
    def save(self, output_path):
        """
        ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            output_path (str or Path): ì €ì¥ ê²½ë¡œ
            
        Returns:
            DataPipeline: ë©”ì„œë“œ ì²´ì´ë‹ì„ ìœ„í•œ self ë°˜í™˜
        """
        if self.df is None:
            raise ValueError("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # is_banmal ì»¬ëŸ¼ ì œê±° (ì„ì‹œ ì»¬ëŸ¼)
        if 'is_banmal' in self.df.columns:
            self.df = self.df.drop('is_banmal', axis=1)
        
        self.df.to_csv(output_path, index=False, encoding='utf-8')
        
        print("\\n" + "=" * 60)
        print("âœ… í˜•íƒœì†Œ ë¶„ì„ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {output_path}")
        print(f"ìµœì¢… í–‰ ìˆ˜: {len(self.df)}")
        
        return self
