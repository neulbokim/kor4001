#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
03. Morph í›„ì²˜ë¦¬

Bareun íƒœê¹… ê²°ê³¼ì—ì„œ ì¢…ê²°ì–´ë¯¸/ë¬¸ì¥ë¶€í˜¸/ê¸°í˜¸ë¥¼ ì¶”ì¶œí•˜ì—¬
`morph_results`ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

Input: data/processed/all_communities_tagged.csv
Output: data/processed/all_communities_morph.csv
"""

import json
import sys
from pathlib import Path

import pandas as pd
from tqdm import tqdm

project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from utils.morph_analyzer import MorphAnalyzer


import argparse
import sys

# Interactive decision cache
# Key: (morph, tag), Value: 'EF' (change) or 'KEEP' (keep) or 'SKIP' (skip all)
decision_cache = {}
stats = {'corrections': 0}

def interactive_callback(token, context_tokens, index, sentence=None):
    """
    MorphAnalyzerì—ì„œ í˜¸ì¶œí•˜ëŠ” ëŒ€í™”í˜• ì½œë°± í•¨ìˆ˜
    """
    morph, tag, prob = token[0], token[1], token[2]
    cache_key = (morph, tag)
    
    if cache_key in decision_cache:
        decision = decision_cache[cache_key]
        if decision == 'KEEP':
            return None
        elif decision == 'SKIP':
            return None
        else:
            # decision is the new tag (e.g., 'EF', 'JX')
            stats['corrections'] += 1
            return decision

    # Show context
    start = max(0, index - 3)
    end = min(len(context_tokens), index + 4)
    context_str = ""
    for i in range(start, end):
        t = context_tokens[i]
        t_str = f"{t[0]}/{t[1]}"
        if i == index:
            t_str = f"[{t_str}]"
        context_str += t_str + " "
    
    print(f"\n[Interactive Check] Ambiguous Token: '{morph}' ({tag}) prob={prob:.4f}")
    if sentence:
        print(f"Full Sentence: \"{sentence}\"")
    print(f"Context: ... {context_str} ...")
    
    while True:
        print("Options:")
        print("  [e] Change to EF")
        print("  [k] Keep (Don't change)")
        print("  [c] Custom Tag")
        print("  [d] Delete Sentence (Remove from dataset)")
        print("  [s] Skip (Don't ask again for this morph)")
        print("  Add 'a' to apply to all (e.g., 'ea', 'ka', 'ca')")
        
        choice = input("Choice: ").strip().lower()
        
        apply_all = False
        if len(choice) > 1 and choice.endswith('a'):
            apply_all = True
            choice = choice[:-1]
            
        if choice == 'e':
            new_tag = 'EF'
            if apply_all:
                decision_cache[cache_key] = new_tag
            stats['corrections'] += 1
            return new_tag
            
        elif choice == 'k':
            if apply_all:
                decision_cache[cache_key] = 'KEEP'
            return None
            
        elif choice == 'd':
            # Delete sentence
            stats['deletions'] = stats.get('deletions', 0) + 1
            return 'DELETE'
            
        elif choice == 's':
            decision_cache[cache_key] = 'SKIP'
            return None
            
        elif choice == 'c':
            new_tag = input("Enter new tag (e.g., JX, MAG): ").strip().upper()
            if not new_tag:
                print("Invalid tag.")
                continue
            if apply_all:
                decision_cache[cache_key] = new_tag
            stats['corrections'] += 1
            return new_tag
            
        else:
            print("Invalid choice.")

def main():
    parser = argparse.ArgumentParser(description="Morph í›„ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--interactive", action="store_true", help="ëŒ€í™”í˜• ëª¨ë“œë¡œ ì‹¤í–‰í•˜ì—¬ ì• ë§¤í•œ íƒœê·¸ë¥¼ ì§ì ‘ í™•ì¸í•©ë‹ˆë‹¤.")
    parser.add_argument("--gallery", type=str, help="íŠ¹ì • ê°¤ëŸ¬ë¦¬/ì»¤ë®¤ë‹ˆí‹°ë§Œ ì²˜ë¦¬ (íŒŒì¼ëª…ì— í¬í•¨ëœ ë¬¸ìì—´)")
    args = parser.parse_args()

    print("=" * 60)
    print("Morph í›„ì²˜ë¦¬ ì‹œì‘")
    if args.interactive:
        print("ğŸ“¢ ëŒ€í™”í˜• ëª¨ë“œ í™œì„±í™”: í™•ë¥  0.92 ì´í•˜ì¸ EC íƒœê·¸ì— ëŒ€í•´ í™•ì¸ì„ ìš”ì²­í•©ë‹ˆë‹¤.")
    if args.gallery:
        print(f"í•„í„° ì ìš©: '{args.gallery}'")
    print("=" * 60)

    input_dir = Path("data/processed/tagged")
    output_dir = Path("data/processed/morph")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    all_files = list(input_dir.glob("tagged_*.csv"))
    if args.gallery:
        input_files = [f for f in all_files if args.gallery in f.name]
    else:
        input_files = all_files
    
    if not input_files:
        print(f"\nâŒ ì˜¤ë¥˜: ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (í•„í„°: {args.gallery})")
        return

    print(f"ë°œê²¬ëœ íŒŒì¼: {len(input_files)}ê°œ")
    analyzer = MorphAnalyzer()

    for input_path in input_files:
        print(f"\nì²˜ë¦¬ ì¤‘: {input_path.name}")
        
        community_name = input_path.stem.replace("tagged_", "")
        output_filename = f"morph_{community_name}.csv"
        output_path = output_dir / output_filename
        
        df = pd.read_csv(input_path)
        
        # Deduplication: Remove rows with duplicate 'sentence' (if exists) or 'content'
        # Check available columns
        target_col = 'content' if 'content' in df.columns else 'sentence'
        if target_col in df.columns:
            initial_len = len(df)
            df.drop_duplicates(subset=[target_col], inplace=True)
            removed_len = initial_len - len(df)
            if removed_len > 0:
                print(f"  - ì¤‘ë³µ ì œê±°ë¨: {removed_len}ê°œ í–‰ (ê¸°ì¤€: {target_col})")
        
        processed_rows = []
        
        # Use callback only if interactive mode is on
        callback = interactive_callback if args.interactive else None
        
        # Incremental saving setup
        save_interval = 100
        output_path_partial = output_path.with_suffix('.partial.csv')
        
        # If resuming (partial file exists), load it to count processed rows
        start_idx = 0
        if output_path_partial.exists():
            try:
                partial_df = pd.read_csv(output_path_partial)
                start_idx = len(partial_df)
                print(f"  ğŸ”„ ì´ì „ ì‘ì—… ë°œê²¬: {start_idx}ê°œ í–‰ ì²˜ë¦¬ë¨. ì´ì–´ì„œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            except Exception as e:
                print(f"  âš ï¸ ë¶€ë¶„ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ (ìƒˆë¡œ ì‹œì‘): {e}")
        
        # Slice df to skip processed rows
        if start_idx > 0:
            if start_idx >= len(df):
                print("  âœ… ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                # Rename partial to final if needed?
                # If final doesn't exist but partial does and is complete.
                if not output_path.exists():
                    output_path_partial.replace(output_path)
                    print(f"  -> ì €ì¥ ì™„ë£Œ: {output_path}")
                continue
            
            df = df.iloc[start_idx:]
            print(f"  -> ë‚¨ì€ {len(df)}ê°œ í–‰ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f"{community_name} Morph"):
            segments = []
            try:
                segments = json.loads(row.get('sentence_segments', '[]') or '[]')
            except json.JSONDecodeError:
                segments = []
    
            morph_results = []
            skip_row = False
            
            for segment in segments:
                base_sentence = segment.get('sentence', '')
                tokens = segment.get('tokens', [])
                
                # Pass callback to segment_sentence_by_endings
                try:
                    segmented = analyzer.segment_sentence_by_endings(base_sentence, tokens, refinement_callback=callback)
                except Exception as e:
                    print(f"Error processing row {idx}: {e}")
                    continue
                    
                for piece_text, piece_tokens in segmented:
                    # Check for DELETE signal
                    if any(t[1] == 'DELETE' for t in piece_tokens):
                        skip_row = True
                        break
                        
                    ending_tokens = analyzer.extract_final_endings(piece_tokens)
                    punctuation, other_symbols = analyzer.extract_symbols(piece_text)
    
                    # Calculate min probability
                    probs = [t[2] for t in piece_tokens if len(t) >= 3]
                    min_prob = min(probs) if probs else 0.0
                    last_token_prob = piece_tokens[-1][2] if piece_tokens and len(piece_tokens[-1]) >= 3 else 0.0
                    
                    # Check for OOV
                    has_oov = any(len(t) >= 4 and t[3] > 0 for t in piece_tokens)
                    
                    # Manual intent check logic (simplified)
                    needs_manual_intent = False
                    if min_prob < 0.95 or has_oov:
                        needs_manual_intent = True
    
                    morph_results.append({
                        "sentence": piece_text,
                        "endings": ending_tokens,
                        "punctuation": punctuation,
                        "other_symbols": other_symbols,
                        "min_prob": min_prob,
                        "last_token_prob": last_token_prob,
                        "has_oov": has_oov,
                        "needs_manual_intent": needs_manual_intent
                    })
    
            if skip_row:
                continue

            if not morph_results:
                morph_results.append({
                    'sentence': row.get('full_text', ''),
                    'endings': [],
                    'punctuation': [],
                    'other_symbols': [],
                    'min_prob': 1.0,
                    'last_token_prob': 1.0,
                    'has_oov': False,
                    'needs_manual_intent': True
                })
    
            row['morph_results'] = json.dumps(morph_results, ensure_ascii=False)
            processed_rows.append(row)
            
            # Incremental saving
            if len(processed_rows) >= save_interval:
                partial_df = pd.DataFrame(processed_rows)
                # Append to partial file
                header = not output_path_partial.exists()
                partial_df.to_csv(output_path_partial, mode='a', header=header, index=False, encoding='utf-8-sig')
                processed_rows = [] # Clear buffer
        
        # Save remaining rows
        if processed_rows:
            partial_df = pd.DataFrame(processed_rows)
            header = not output_path_partial.exists()
            partial_df.to_csv(output_path_partial, mode='a', header=header, index=False, encoding='utf-8-sig')
        
        # Rename partial to final
        if output_path_partial.exists():
            output_path_partial.replace(output_path)
            print(f"  -> ì €ì¥ ì™„ë£Œ: {output_path}")
        else:
            print("  -> ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print("\n" + "=" * 60)
    print("âœ… Morph í›„ì²˜ë¦¬ ì™„ë£Œ")
    if args.interactive:
        print(f"ğŸ“Š ëŒ€í™”í˜• ìˆ˜ì • ì™„ë£Œ: ì´ {stats['corrections']}ê°œ íƒœê·¸ ë³€ê²½ë¨")
    print("=" * 60)


if __name__ == "__main__":
    main()
